<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,target-densitydpi=device-dpi">
<meta name="generator" content="verse">
<meta name="X-UA-Compatible" content="IE=edge">


<title>SHIELD</title>


<link rel="icon" href="/favicon.ico">
<link rel="stylesheet" href="/site.css?d51129b9dca47a3183236d6f49a533636168b196">
<link rel="alternate home" type="application/rss+xml" title="SHIELD - RSS feed" href="/feed.xml">

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50822525-7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js',new Date());gtag('config','UA-50822525-7');</script>
</head>
<body>
  <header class="top">
    <img src="/logo.png?6eed3f85a6b3b055aec7dfb67c8cd7129acc1ab7">
    <nav>
      <li><a href="/">Home</a></li>
      <li><a href="/download/">Download</a></li>
      <li><a href="/community/">Community</a></li>
      <li><a href="/docs/latest/ops/">Docs</a></li>
      <!--li><a href="/docs/latest/dev/">Developers</a></li-->
    </nav>
  </header>

    <div class="header">
    <div>
      <h1>SHIELD Roadmap Call</h1>
      
    </div>
  </div>

  <div class="content">
    <div>
      <nav>
        <h2>Coming Up</h2>
        <li><a href="#next"></a></li>
        <h2>Previous Calls</h2>
        
        <li><a href="#20200528">May 28th, 2020</a></li>
        
        <li><a href="#20200123">January 23rd, 2020</a></li>
        
        <li><a href="#20190926">September 26th, 2019</a></li>
        
        <li><a href="#20190822">August 22nd, 2019</a></li>
        
        <li><a href="#20180913">September 2018</a></li>
        
        <li><a href="#20180809">August 2018</a></li>
        
        <li><a href="#20180712">July 2018</a></li>
        
      </nav>
    </div>

    <div>
      <h1>Joining the Call</h1>

<p>On the fourth Thursday of every month, we hold a conference call to discuss
the SHIELD project, and its overall direction.  Topics include latest news,
development progress, and future direction.</p>

<p>The next SHIELD Roadmap Call is
<br>
<strong>Thursday, June 25th</strong> at <strong>11:00am EDT</strong>.</p>

<p>We use zoom: <a href="https://us02web.zoom.us/j/7165551212">https://us02web.zoom.us/j/7165551212</a></p>

<p>Calls are not recorded.</p>

<h1>Previous Calls</h1>

<p>We don't record the roadmap calls, but we do take copious amounts of notes
and attempt to summarize them below.  Wouldn't your rather skim a report
<em>on</em> a meeting, rather than watch a static recording <em>of</em> the meeting??</p>

<h2 id="20200528">May 28th, 2020</h2>

<p>These are the notes from our sixth community call, held at 11:00am EDT on
Thursday, May 28th, 2020.</p>

<h2>The Future of SHIELD</h2>

<p>Most of this Roadmap call was devoted to the discussion of structural cracks
and flaws in the core architecture of SHIELD, and of systemic failures in
past choices.  The world moves on, technology improves, and sometimes the
ideas that seemed good at the time become worse, until they are no longer
viable.</p>

<p>To that end, we are planning an ambitious, unfunded open source push to fix
most or all of these problems.  Namely:</p>

<ol>
<li>Replace Tenants with Folders</li>
<li>Vue.js for the Web UI (dropping lens.js / AEGIS)</li>
<li>A Better CLI Experience</li>
<li>Externalize the Vault</li>
<li>Externalize the Database</li>
<li>Externalize the Scheduler</li>
<li>SSH Active Fabric</li>
</ol>

<h3>Tenants will be replaced with Folders</h3>

<p>Few people we have spoken to actually use tenancy, and the leakiness of the
ACLs involved contributed to that.  In the end, those who <em>were</em> using
tenancy were doing so for organizing systems into groups / categories /
taxonomies (i.e. all of the production databases, this CF, that K8s)</p>

<h3>Vue.js for the Web UI (dropping lens.js / AEGIS)</h3>

<p>We've had nothing but issues (bugs, inconsistencies, inaccuracies) with the
SHIELD Web UI.  While we still think its a useful thing to have, we no
longer believe that it is (a) required or (b) benefits from a non-standard
front-end framework.</p>

<p>So, we'll be rewriting it in Vue.js, with more node-iness than before, and
accompanied by library-level unit tests that are part of CI/CD runs.  This
means that AEGIS goes away (to be replaced by native Vue.js data
management), and the websocket gets an overhaul to accommodate (more on that
later)</p>

<h3>A Better CLI Experience</h3>

<p>The CLI experience for SHIELD needs smoothed out.  Our new goal is to be
able to dispense entirely with the UI, and use the CLI for everything, if we
so choose.</p>

<h3>Externalize the Vault</h3>

<p>SHIELD will no longer manage its own Vault.  When we first started this, it
was easier and better for us to ship our own Vault instance, dedicated to
SHIELD.  But in the modern era of Helm charts and other automation, it's
much easier to properly set up a Vault.  Plus, most people are already
running one anyway.</p>

<p>Removing the responsibility of managing the Vault from SHIELD will simplify
our codebase, and make deployments more flexible.  <em>Distributions</em> of SHIELD
may opt to continue running their own dedicated Vault process, at their
discretion.</p>

<h3>Externalize the Database</h3>

<p>SQLite doesn't scale, not the way we're using it.  This change moves forward
on two fronts: getting task logs out of the database proper, and switching
to a database-agnostic ORM layer like <code>gorm</code> (which has come a long way since
we first evaluated it).  We don't have hefty performance requirements from
our database layer, and programming effort (and complexity!) should be
concentrated elsewhere.</p>

<p>Our initial target is PostgreSQL (which is about as easy to spin as SQLite
these days anyway), but if we end up effortlessly supporting other RDBMSes
via an ORM, all the better.</p>

<h3>Externalize the Scheduler</h3>

<p>Currently, we cannot scale the SHIELD Core, either for availability or
performance reasons, because the scheduler is entirely in memory.  To
support horizontal scale-out, we will be externalizing the state of the
scheduler into something like Redis (or the database) to allow multiple
cores to operate concurrently.</p>

<h3>SSH Active Fabric</h3>

<p>The time has come to solve the NAT problem, and allow SHIELD agents from
behind a network address translation scheme to work with a core on the other
side.  To do this, we will be flipping the direction of communication, and
using the <a href="https://github.com/jhunt/go-sfab">sfab</a> library to do it.</p>

<p>Under this new scheme, SHIELD agents will SSH <em>into</em> the SHIELD core,
authenticate themselves and await instructions.  The SHIELD core will then
hand out tasks (dispatched via the sfab framework) to agents, or skip task
scheduling for unknown / unreachable agents.</p>

<p>This is a very large shift, architecturally, and it will guarantee that we
are not able to handle backwards compatibility with SHIELD v8.</p>

<h2>The v9 Project</h2>

<p>Given all the change we want to make, we are going to break with the v8.x
series and implement SHIELD v9 as a new system, with similar concepts and
the same supported data systems.  This new SHIELD will not be a drop-in
replacement for v8.x and prior releases.  As such we expect to be able to
move forward more quickly with the large changes we are about to implement.</p>

<p>Work will proceed under a v9 branch.  This work will be 100% open source,
volunteer-driven.  We will coordinate effort via the GitHub issue tracker.</p>

<h2>K8s, SIGs, and Go Libraries</h2>

<p>We've been watching the core teams in the Kubernetes project for a while
now, and their federated approach to both code and governance seems to be
working out quite well.  While we don't have the same requirements of a
governance structure (BDFL seems to be working), we will begin to emulate
the federate code structure by pulling parts of SHIELD Core implementation
into standalone(-ish) code bases that act as libraries.</p>

<p>When this is all said and done, the core binary as we know it today will
mostly be glue code, implementing the API and data persistence layer, but
otherwise relying on the libraries of SHIELD and SHIELD-like functionality
to do all the heavy lifting.</p>

<p>This (we believe) will grant us the ability to test some obscure edge cases
that are particularly tricky to reproduce with a full complement of SHIELD
core, storage system, agent, and target system.  Our goal is to get 80% or
better C0 code coverage during an extraction port, but more importantly to
have a harness in place for writing regression tests for library-bound bugs,
going forward.</p>

<p>Work has been proceeding, in skunkworks projects, on this front for a month
or so now, and the results are promising.</p>

<h2>Kubernetes Controller + CRDs</h2>

<p>Extracting core functionality into one or more (tested) libraries also
enables us to experiment with other substrates for the ideas behind SHIELD.
Namely, we'll be able to quickly experiment with Kubernetes, by turning the
database objects into Kubernetes Custom Resources, and swapping out our API
for that of the Kubernetes API.</p>

<h2 id="20200123">January 23nd, 2020</h2>

<p>These are the notes from our fifth community call, held at 11:00am EDT on
Thursday, January 23nd, 2020.</p>

<p><strong>Happy New Year!</strong></p>

<h2>Spring 2020 Roadmap Goals</h2>

<p>Our main goals for SHIELD in 2020:</p>

<ol>
<li><p>Scalability:</p>

<ul>
<li><p>We currently are hitting a ulimit of about 180 agents.</p></li>
<li><p>We will be exploring Postgres as a replacement for Sqlite to better
handle the database lockout problem with lots of agents.</p></li>
<li><p>We will be looking to stop writing task logs to the database to further
reduce strain on the database.</p></li>
<li><p>One of our goals is HA SHIELD. (Externalize vault/scheduler).</p></li>
<li><p>Active ssh fabric for agents.</p></li>
</ul></li>
<li><p>Containers:</p>

<ul>
<li><p>Continued improvements with SHIELD on k8s via helm.</p></li>
<li><p>Move bosh release to the containers release + SHIELD docker image.</p></li>
<li><p>Changes to CI/CD to reflect new packaging </p></li>
</ul></li>
</ol>

<p>After reviewing our roadmap we had discussions about some of the obstacles that
we need to overcome to accomplish our goals for 2020. We talked at length about how
we should best handle our transition from Sqlite to Postgres.</p>

<h2 id="20190822">August 22nd, 2019</h2>

<p>These are the notes from our fourth community call, held at 11:00am EDT on
Thursday, August 22nd, 2019.</p>

<h2>Rebooting the Roadmap Call</h2>

<p>Sometimes life happens, and gets in the way of other things.  We're okay now
though, so the SHIELD Team is going to resume the Roadmap Calls monthly.</p>

<h2>SHIELD v8.4 is out &mdash; v8.5 On The Horizon</h2>

<p>The SHIELD Team released <a href="v8.4">version 8.4</a> of SHIELD on August 9th, and is
busily preparing for a mid-September release of <a href="v8.5-mile">version 8.5</a>.</p>

<h2>etcd!</h2>

<p>shieldproject sponsored an internship over the Summer of 2019 in which four
enterprising University of Buffalo CS Students built a backup plugin for
CoreOS' etcd key-value system.  Congratulations to Sriniketh, Pururva,
Jason, and Naveed!</p>

<h2>Website Redesign</h2>

<p>This is our website, but you may not recognize it.  We just got a nifty
new redesign.  Now that the day-to-day annoyance of modifying the website
content is more well-in-hand, the team expects to start putting up more and
more blog posts and documentation!</p>

<h2>Blogs, Blogs, and More Docs!</h2>

<p>The SHIELD Team has been putting forth a concerted effort to write more
approachable "first timer" content to get people interested in deploying
SHIELD to protect their valuable cloud data assets.</p>

<p>Hopefully, by the time of our next Community Call (September 26th), we'll
have something more concrete to show off.</p>

<h2 id="20180913">September 13th, 2018</h2>

<p>These are the notes from our third community call, held at 11:00am EDT on
Thursday, September 13th, 2018.</p>

<h3>New BackBlaze Storage Plugin</h3>

<p>Thanks to the efforts of Mr. Dan Molik (shieldproject), the SHIELD
project now has support for storing backup archives in BackBlaze's B2
endpoint, via the new <code>b2</code> storage plugin!</p>

<h3>Download (and re-encryption) of Backup Archives</h3>

<p>The SHIELD Team is working on building new functionality into SHIELD for
(securely) downloading archives from Cloud Storage, via the web interface
or the CLI.</p>

<p>This used to be a lot easier in SHIELD v6, but when we introduced
encryption into the mix, we lost the ability to inspect individual
archives.  Furthermore, SHIELD complicates the issue by randomizing the
keys and initialization vectors (IVs) for <em>each archive</em>, and stores them
in the local Vault, which operators do not have access to.  Even for
"fixed-key" backups, the key that the operators are given is used to
<em>derive</em> the key and IV used to encrypt the archives.</p>

<p>To remedy this, and yet maintain the security of archives, SHIELD is
being extended to include a new API endpoint for downloading and
re-encrypting a single archive (subject to tenant rights and access
control).  This new endpoint will take encryption parameters from the
operator, and re-encrypt the archive as it is streamed from storage.</p>

<h3>Open Forum</h3>

<p>Jordan and Phillip asked several very good questions.  Good on them!</p>

<p><strong>Smoking Hole SHIELD Restores</strong> - <em>Do we have documentation for the
recovery of a SHIELD core itself?</em></p>

<p>We have a process, and it's sort of documented, but not well.  We're
working on fleshing out the <a href="/docs/latest/ops/manual">SHIELD Operations Manual</a>
with these details.</p>

<p><strong>Authentication Tokens</strong> - <em>Authentication Tokens seem to be tied to
individual SHIELD user accounts.  Do those persist across SHIELD
re-deployments?</em></p>

<p>Yes, Authentication Tokens are tied to individual accounts.  Behind the
scenes, those are implemented as non-expiring sessions associated with
the user who created them.</p>

<p>Because they are resident in the sessions table in the database, they
should persist across BOSH deploys, assuming (a) you are using
persistent disks and (b) you are not deleting the BOSH deployment
first.</p>

<p><strong>Bootstrapping Local Users</strong> - <em>Can The SHIELD BOSH Release Bootstrap
Local Users?</em></p>

<p>No it cannot, but it should.</p>

<p><strong>Mutual Network Visibility of SHIELD Agents</strong> - <em>In v6, the only
communication channel was unidirectional, wherein the SHIELD core
connected to each SHIELD agent to orchestrate a backup.  This does not
seem to be the case in v8.</em></p>

<p>This isn't really a question, but we'll let that slide.</p>

<p>In v8, the SHIELD core still initiates an SSH session to the agent
involved to orchestrate it.  What's new in v8 is an HTTPS registration
ping from the agents to the core.  This "ping" puts the agent in the
SHIELD core's database, so that the core can SSH back into the agent
and retrieve metadata from them.</p>

<p>This poses a severe problem with NAT'ed installations which lack the
required <em>mutual network visibility</em> to make this work.  What we (ShieldProject)
have been doing is just colocating the SHIELD core in the same
NAT "scope" as the systems being orchestrated.</p>

<p>We've designed a few alternatives to this.  One is a SHIELD-aware NAT
traversal proxy that would handle connections to/from the agents across
the NAT boundary.  This is a fair bit of engineering work, but it does
preserve backwards compatibility.</p>

<p>A far superior solution is to break backwards compatibility, and invert
the orchestration flows.  If the SHIELD agents SSH into the SHIELD
core, announce themselves, and then await orders, we can handle both
the metadata retrieval / agent discovery, and traverse NATs with ease.</p>

<h2 id="20180809">August 9th, 2018</h2>

<p>These are the notes from our second community call, held at 11:00am EDT on
Thursday, August 9th, 2018.</p>

<h3>Lean Table View</h3>

<p>SHIELD now supports a Lean Table View, for collapsing lots of cards into
a more compact, tabular view.  We'll be working on making that view also
available on the wizards (configure backup, run ad hoc, and restore).</p>

<h3>Optional Compression</h3>

<p>Bzip2 Compression of archives is now optional.  The implementation makes
it modular, so we can add in new compression schemes (gzip, zip, lzma,
etc.)</p>

<h2>Plugin Reference Documentation</h2>

<p>We've started documenting the <a href="/docs/latest/ops/plugins">SHIELD Plugins</a> and
their configuration.</p>

<p>Thanks all for joining!  See you next month!</p>

<h2 id="20180712">July 12th, 2018</h2>

<p>These are the notes for our first ever community call, held at 11:00am EDT
on Thursday, July 12th, 2018.</p>

<h3>The Website</h3>

<p>We have a website at <a href="https://shieldproject.io">https://shieldproject.io</a>.</p>

<p>We are putting all of our documentation, guides, and manuals there</p>

<p>Work is underway on fleshing out the
<a href="/docs/latest/ops/manual">SHIELD Operator's Manual</a>, and the
<a href="/docs/latest/ops/plugins">Plugin Reference</a>.</p>

<p>We're also hoping to build out recipe-based docs like
<em>How do I backup Cloud Foundry?</em></p>

<h3>The Trello Board</h3>

<p>We have started using a <a href="https://trello.com/b/qLZCvexK/shield">Trello Board</a> for coordinating developer
activity.  If you want to get involved in hacking on SHIELD, ask in
the #help channel on <a href="/community#slack">Slack</a>.</p>

<p>GitHub Issues will still be the place to report bugs and ask for feature
requests.  Trello is more for things that require multiple different
states, prioritization, backlog management, etc.</p>

<p>Every month we will be highlighting delivered features and fixed bugs
that we feel are important enough to announce (big, operator-visible
stuff), and discussing our focus for the next month.</p>

<h3>Current Future Direction</h3>

<p>We've got a lot of minor bugs in the Trello backlog; we're focusing on
those first, to get them fixed and get the fixes shipped.</p>

<p>We've also conducted an internal review, which we're calling <em>Gap
Analysis</em>.  Put simply, it's all the things we haven't finished, and all
the features we know we need but haven't implemented.  This includes
things like being able to edit systems from the web interface, viewing
tasks, rescheduling backup jobs, etc.</p>

<p>Once we've got the bug backlog tamed, we'll be moving onto the gap
tickets.</p>

<h3>Release Cadence</h3>

<p>We're hoping to hit a weekly release schedule with SHIELD, the SHIELD
BOSH release, and the SHIELD Genesis Kit.  Our current plan is to cut new
point releases on Friday afternoons.</p>

<h3>Open Forum</h3>

<p>Jordan asked a bunch of questions.  Thanks, mate!</p>

<p><strong>CLI Help Documentation</strong> - <em>Curious whether or not we were going to
provide more documentation for CLI usage, either via the website, or
inline via <code>--help</code> flags.</em></p>

<p>Short answer: yes.</p>

<p>Long answer: definitely yes, via both methods.  We'll review the state
of SHIELD help inside the CLI, and see if there are logical places to
flesh out what we've got, cover more abstract topics, etc.</p>

<p><strong>Can SHIELD itself be recovered via the CLI only?</strong></p>

<p>Currently, no.  The SHIELD recovery mechanism (with the fixed key
backup) relies on visual elements that are part of the Web UI.
However, it's a good feature (one we're missing, so it's a "gap"),
and we'll be looking into support for that particular workflow.</p>

<p><strong>Manual Archive Management</strong> - <em>Having deleted archives manually from
backing cloud storage, Jordan noticed that the SHIELD Web UI didn't
update its storage usage counts.</em></p>

<p>This is normal, and we'll address it specifically in the documentation,
since SHIELD expects to have full and complete control over cloud
storage.  We did talk through a UI view (CLI and Web) for browsing cloud
storage archives, and deleting them there, with an eye towards reclaiming
space.</p>

<p><strong>Monitoring and Notifications</strong> - <em>Is it possible to notify about storage
usage limits being reached, failing storage, failing jobs, etc.?</em></p>

<p>Short answer: yes, via the <code>/v2/health</code> endpoint, in an external
monitoring system.</p>

<p>Long answer: we have plans for a separate project / product, for handling
notifications more globally, with business logic and dispatch rules built
into <em>that</em> system, which SHIELD would then integrate with.</p>
    </div>
  </div>

  <footer>
    <div class="g">
      <div class="x3">
        <img src="/swlogo.png?0b567bdcee59ac95e3eaf667ec543ebc1631d6c0">

        <p>SHIELD is maintained by shieldproject,
        a leading Cloud-Native consulting firm.</p>
      </div>

      <div class="x3">
        <nav>
          <li><a href="/">Home</a></li>
          <li><a href="/download/">Download</a></li>
          <li><a href="/community/">Community</a></li>
          <li><a href="/docs/latest/ops/">Docs</a></li>
          <!--li><a href="/docs/latest/dev/">Developers</a></li-->
        </nav>
      </div>

      <div class="x3">
      </div>
    </div>

    <div class="copyright">
      <p>Copyright &copy; 2019 The SHIELD Authors.  All rights reserved.</p>
    </div>
  </footer>
  <script type="text/javascript" src="/site.js?d2112e355882784cf7f38561cf3ccb0bd5da3111"></script>
</body>
</html>
